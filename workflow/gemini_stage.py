import os
import json
import yaml
from dotenv import load_dotenv
import google.generativeai as genai

# Load environment variables from .env file
load_dotenv()

def load_prompt(filepath):
    """Loads a prompt from a file."""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        print(f"Warning: Prompt file not found at {filepath}")
        return ""

def read_chatgpt_outputs(session_dir):
    """Reads all the files generated by the ChatGPT stage."""
    raw_dir = os.path.join(session_dir, "01_raw")

    summary = load_prompt(os.path.join(raw_dir, "summary.md"))

    sources = []
    try:
        with open(os.path.join(raw_dir, "sources.json"), "r", encoding="utf-8") as f:
            sources = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        print("Warning: sources.json not found or is invalid.")

    notes_dir = os.path.join(raw_dir, "notes")
    notes = {}
    if os.path.isdir(notes_dir):
        for filename in os.listdir(notes_dir):
            topic = filename.replace("_", " ").replace(".md", "")
            notes[topic] = load_prompt(os.path.join(notes_dir, filename))

    return summary, sources, notes

def run_gemini_stage(user_task, session_dir, chatgpt_result):
    """
    Runs the Gemini stage to analyze collected data and generate the final study.
    """
    print("Running Gemini stage...")

    # 1. Load configuration
    with open("config/gemini_config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    # 2. Configure the Gemini client
    try:
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
    except Exception as e:
        print(f"Error configuring Gemini API: {e}")
        # Return error structure
        return "Error: Could not configure Gemini API.", {
            "model": config.get("model", "unknown"), "used_k": 0, "max_k": 0
        }

    # 3. Load data from the ChatGPT stage
    summary, sources, notes = read_chatgpt_outputs(session_dir)

    # 4. Load Gemini's prompts
    system_instruction = (
        load_prompt("prompts/gemini_system.md") +
        "\\n\\n" +
        load_prompt("prompts/gemini_format.md")
    )

    # 5. Construct the full prompt for the API
    user_prompt_content = (
        f"Here is the research on the topic '{user_task}'. Please generate the final study based on the provided data.\\n\\n"
        f"--- SUMMARY ---\\n{summary}\\n\\n"
        f"--- SOURCES ---\\n{json.dumps(sources, indent=2)}\\n\\n"
        f"--- DETAILED NOTES ---\\n"
    )
    for topic, content in notes.items():
        user_prompt_content += f"Topic: {topic}\\n{content}\\n\\n"

    # 6. Initialize the model and call the API
    model = genai.GenerativeModel(
        model_name=config["model"],
        system_instruction=system_instruction
    )

    result = ""
    usage = {}

    try:
        response = model.generate_content(user_prompt_content)
        result = response.text

        # Note: As of late 2023, Gemini API via this SDK does not return detailed
        # token counts in the 'response' object itself. This is a known limitation.
        # We will estimate or leave it as 0 for now.
        usage = {
            "model": config["model"],
            "used_k": 0,  # Placeholder
            "max_k": config["max_context_tokens"] / 1000,
            "used_tokens": 0, # Placeholder
            "max_context_tokens": config["max_context_tokens"],
        }

    except Exception as e:
        print(f"An error occurred while calling the Gemini API: {e}")
        result = "Error: Could not generate the final study from Gemini API."
        usage = {
            "model": config.get("model", "unknown"),
            "used_k": 0,
            "max_k": config.get("max_context_tokens", 0) / 1000,
            "used_tokens": 0,
            "max_context_tokens": config.get("max_context_tokens", 0),
        }

    # 7. Save the final study
    final_dir = os.path.join(session_dir, "03_final")
    os.makedirs(final_dir, exist_ok=True)
    with open(os.path.join(final_dir, "estudo_final.md"), "w", encoding="utf-8") as f:
        f.write(result)

    return result, usage
